{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caiowmp/UFBA/blob/master/C%C3%B3pia_de_P03_Classificando_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificando Tweets"
      ],
      "metadata": {
        "id": "vqWJsf-V7zST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Defina o termo que você deseja buscar no Twitter. Pense em algo interessante em termos de classificação de uma postagem. \n",
        "\n",
        "Por exemplo, você pode procurar por Twittes que contenham palavras com possemia, tal como banco, que pode ser uma instituição financeira ou um assento. \n",
        "\n",
        "Você também pode construir buscas mais avançadas, utilizando os operadores disponíveis na API do Twitter (https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators)"
      ],
      "metadata": {
        "id": "wlW9XJk676qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'coxinha lang:pt'"
      ],
      "metadata": {
        "id": "by1Ga5hhtHdz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Utilize o seu BEARER_TOKEN para realizar as consultas na API do Twitter."
      ],
      "metadata": {
        "id": "HYkvsRZe8hIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use o seu próprio BEARER_TOKEN\n",
        "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAEF1gQEAAAAATxx2Gqt0DsT8B%2FMmkbTqLWLxhxI%3DHEvHDuMr50dJLnZIPkfx5pnbW6iE3cf5rKFuAHor3AQEqJtV2L'"
      ],
      "metadata": {
        "id": "8uNBt2Sp8qOW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - O código abaixo não precisa ser aberto nem alterado."
      ],
      "metadata": {
        "id": "oMD6PbL29Joa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wW36gy-RKkzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d5daf4-fbff-4a22-af89-222ed6ef565a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ],
      "source": [
        "#@title Mantenha esse código oculto\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import csv\n",
        "import requests\n",
        "import pandas\n",
        "import time\n",
        "from requests.utils import quote\n",
        "# import emoji\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "%matplotlib inline\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "\n",
        "def busca_frequencia(palavras):\n",
        "  palavras = nltk.FreqDist(palavras)\n",
        "  return palavras\n",
        "\n",
        "def busca_palavras(frases):\n",
        "  todas_palavras = []\n",
        "  for (palavras,sentimento) in frases:\n",
        "    todas_palavras.extend(palavras)\n",
        "  return todas_palavras\n",
        "\n",
        "def aplica_stemmer(texto,lista_stop):\n",
        "  stemmer = nltk.stem.RSLPStemmer()\n",
        "  frases_sem_stemming = []\n",
        "  for (palavras,sentimento) in texto:\n",
        "    com_stemming = [str(stemmer.stem(p)) for p in palavras if p not in lista_stop]\n",
        "    frases_sem_stemming.append((com_stemming,sentimento))\n",
        "  return frases_sem_stemming\n",
        "\n",
        "def busca_palavras_unicas(frequencia):\n",
        "  freq = frequencia.keys()\n",
        "  return freq\n",
        "\n",
        "def remove_stopwords(texto,lista_stop):\n",
        "  frases = []\n",
        "  for (palavras,sentimento) in texto:\n",
        "    sem_stop = [p for p in palavras.split() if p not in lista_stop]\n",
        "    frases.append((sem_stop,sentimento))\n",
        "  return frases\n",
        "\n",
        "def analizador(tweet,base_treinamento):\n",
        "\n",
        "  # exemplo_base = pd.DataFrame(base_treinamento)\n",
        "  # exemplo_base.columns = ['Frase','Sentimento']\n",
        "  # print('Tamanho da base de treinamento {}.',format(exemplo_base.shape[0]))\n",
        "  # print(exemplo_base.Sentimento.value_counts())\n",
        "  # print((exemplo_base.Sentimento.value_counts()/exemplo_base.shape[0])*100)\n",
        "  # print(exemplo_base.sample(n=3))\n",
        "\n",
        "  lista_stop = nltk.corpus.stopwords.words('portuguese')\n",
        "  # lista_stop.append('tão')\n",
        "  # print(np.transpose(lista_stop))\n",
        "\n",
        "  base_treinamento = remove_stopwords(base_treinamento,lista_stop)\n",
        "  # for b in base_treinamento:\n",
        "  #   print(b)\n",
        "\n",
        "  frases_com_stem_treinamento = aplica_stemmer(base_treinamento,lista_stop)\n",
        "  palavras_treinamento = busca_palavras(frases_com_stem_treinamento)\n",
        "  frequencia_treinamento = busca_frequencia(palavras_treinamento)\n",
        "  palavras_unicas_treinamento = busca_palavras_unicas(frequencia_treinamento)\n",
        "\n",
        "  def extrator_palavras(documento):\n",
        "    doc = set(documento)\n",
        "    caracteristicas = {}\n",
        "    for palavras in palavras_unicas_treinamento:\n",
        "      caracteristicas['%s' %palavras] = (palavras in doc)\n",
        "    return caracteristicas\n",
        "\n",
        "  base_completa_treinamento = nltk.classify.apply_features(extrator_palavras,frases_com_stem_treinamento)\n",
        "  classificador = nltk.NaiveBayesClassifier.train(base_completa_treinamento)\n",
        "  # print(classificador.labels())\n",
        "\n",
        "  tweets = [tweet]\n",
        "\n",
        "  for t in tweets:\n",
        "    testeStemming = []\n",
        "    stemmer = nltk.stem.RSLPStemmer()\n",
        "    for (palavras_treinanemto) in t.split():\n",
        "      comStem = [p for p in palavras_treinanemto.split()]\n",
        "      testeStemming.append(str(stemmer.stem(comStem[0])))\n",
        "    novo = extrator_palavras(testeStemming)\n",
        "\n",
        "    distribuicao = classificador.prob_classify(novo)\n",
        "    for classe in distribuicao.samples():\n",
        "      print('%s: %f' % (classe,distribuicao.prob(classe)))\n",
        "\n",
        "\n",
        "def create_headers(bearer_token):\n",
        "    \"\"\" Creates headers for requests \"\"\"\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    return headers\n",
        "\n",
        "def create_url(query_field):\n",
        "    \"\"\" Creates a API URL for reuqests \"\"\"\n",
        "    url = f'https://api.twitter.com/2/tweets/search/recent?query={query_field}'\n",
        "    return url\n",
        "\n",
        "def connect_to_endpoint(url, headers):\n",
        "    \"\"\" Connects to API URL endpoint \"\"\"\n",
        "    for i in range(1, 14):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "        except Exception as e:\n",
        "      \n",
        "            print(\"Connection failed, retrying...\",i)\n",
        "            time.sleep(5)\n",
        "\n",
        "def json_extract(obj, key):\n",
        "    \"\"\"Recursively fetch values from nested JSON.\"\"\"\n",
        "    arr = []\n",
        "\n",
        "    def extract(obj, arr, key):\n",
        "        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n",
        "        if isinstance(obj, dict):\n",
        "            for k, v in obj.items():\n",
        "                if isinstance(v, (dict, list)):\n",
        "                    extract(v, arr, key)\n",
        "                elif k == key:\n",
        "                    arr.append(v)\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "                extract(item, arr, key)\n",
        "        return arr\n",
        "\n",
        "    values = extract(obj, arr, key)\n",
        "    return values\n",
        "\n",
        "def main(token):\n",
        "    stop = False\n",
        "    tweets = set()\n",
        "    # for i in tqdm(range(10)):\n",
        "    for i in range(10):\n",
        "        print()\n",
        "        headers = create_headers(BEARER_TOKEN)\n",
        "        queryHTML = quote(query, safe='')\n",
        "        \n",
        "        if token == '':\n",
        "          url = create_url(queryHTML)\n",
        "        else:\n",
        "          url = create_url(queryHTML+'&next_token='+token)  \n",
        "\n",
        "        time.sleep(3.2) # Sleep for 3 seconds\n",
        "        try:\n",
        "            data = connect_to_endpoint(url, headers)\n",
        "        except Exception as e:\n",
        "            print(\"Oh no!!!\")\n",
        "        id = json_extract(data, 'id')\n",
        "        created_at = json_extract(data, 'id')\n",
        "        author_id = json_extract(data, 'author_id')\n",
        "        text = json_extract(data, 'text')\n",
        "\n",
        "        \n",
        "        for t in text:\n",
        "            # print()\n",
        "            # print(t)\n",
        "            # t = t.replace('\\n',' ')\n",
        "            # print()\n",
        "            tweets.add(t)       \n",
        "        \n",
        "        try:\n",
        "            token = json_extract(data, 'next_token')[0]\n",
        "            with open(token+'.json', 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4, sort_keys=True)\n",
        "            # print(token)\n",
        "            break\n",
        "        except:\n",
        "            token = ''\n",
        "            with open('last.json', 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4, sort_keys=True)\n",
        "            print(\"finished\")\n",
        "            stop = True\n",
        "\n",
        "        if stop:\n",
        "            break\n",
        "        # break\n",
        "    \n",
        "    \n",
        "    complete = list(tweets)\n",
        "    return(complete,token)\n",
        "\n",
        "    # hashtagList = []\n",
        "    # for t in complete:\n",
        "    #         for h in re.findall(r\"#(\\w+)\", t):                \n",
        "    #             hashtagList.append(h.lower())\n",
        "    \n",
        "    # hashtagSet = list(set(hashtagList))\n",
        "    # hashtagSet.sort()\n",
        "\n",
        "    # tweetsFile=open('tweets: '+query+'.txt','w')\n",
        "    # for element in complete:\n",
        "    #     tweetsFile.write(element)\n",
        "    #     tweetsFile.write('\\n')\n",
        "    #     tweetsFile.write('\\n')\n",
        "    # tweetsFile.write(str(len(complete)))\n",
        "    # tweetsFile.close()\n",
        "\n",
        "    # hashtagFile=open('hashtag: '+query+'.txt','w')\n",
        "    # for element in hashtagSet:\n",
        "    #     hashtagFile.write(str(hashtagList.count(element)))\n",
        "    #     hashtagFile.write(' ')\n",
        "    #     hashtagFile.write(element)\n",
        "    #     hashtagFile.write('\\n')\n",
        "    # hashtagFile.write(str(len(hashtagSet)))\n",
        "    # hashtagFile.close()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Altere o código abaixo e crie o seu próprio classificador. No exemplo abaixo, estamos classificando os Tweets com a palavra banco como instituição, assento ou indefinido."
      ],
      "metadata": {
        "id": "Cr7k8qb6Hwzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classifica(tweets):\n",
        "  base_treinamento = []\n",
        "  for t in tweets:\n",
        "    print(t)\n",
        "    tipo = input(\"Comida(1), Partido Politico(2), Indefinido(0)?\")\n",
        "    if tipo == '1':\n",
        "      base_treinamento.append((t,'comida'))\n",
        "    elif tipo == '2':\n",
        "      base_treinamento.append((t,'partidoPolitico'))\n",
        "    else:\n",
        "      base_treinamento.append((t,'indefinido'))\n",
        "  return base_treinamento"
      ],
      "metadata": {
        "id": "dj-AiyBiHqQL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - A função principal executa as seguintes tarefas\n",
        "1. Utiliza a chave de busca para coletar tweets\n",
        "1. Utiliza os 10 primeiros tweets coletados para inicializar a base de treinamento\n",
        "1. Continua as coletas e executa a classificação dos próximos Tweets\n",
        "1. Caso a análise não esteja boa, você pode melhora-la.\n",
        "\n"
      ],
      "metadata": {
        "id": "QS_eKSc6JYnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Faz uma coleta inicial de Tweets\n",
        "  token = ''\n",
        "  tweets,token = main(token)  \n",
        "\n",
        "  base_treinamento = []\n",
        "  # Utiliza os primeiros tweets para alimentar a base de treinamento\n",
        "  base_treinamento = classifica(tweets)\n",
        "  print(base_treinamento)\n",
        "\n",
        "  # Continua buscando mais Tweets\n",
        "  while(token != ''):\n",
        "    # Utiliza token para continuar buscando Tweets do último ponto de retorno\n",
        "    tweets,token = main(token) \n",
        "    for t in tweets:\n",
        "      print(t)\n",
        "      analizador(t,base_treinamento)\n",
        "      print()\n",
        "    resposta = input('A analise está boa? s/n')\n",
        "    if resposta == 'n':\n",
        "      print('Vamos classificar melhor esses tweets...')\n",
        "      for i in classifica(tweets):\n",
        "        base_treinamento.append(i)\n"
      ],
      "metadata": {
        "id": "KHzP4kl8rgCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 - Sua tarefa: defina a sua chave de busca e altere o seu código do classificador. Realize seus testes para se certificar que o classificador funciona corretamente. Inclua quantas classes forem necessárias. Ao terminar, faça o download deste Colab e envie através do AVA."
      ],
      "metadata": {
        "id": "L4KUNhyJMVMM"
      }
    }
  ]
}